{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "To run this Jupyter notebook, use Python 3.13 or later. The recommended environment is a virtual environment or a conda environment with the necessary packages installed, as specified int he requirements.txt file. Ensure that you have Jupyter Notebook installed to execute the cells interactively.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run reuse.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a dropdown widget for environment selection\n",
    "env_dropdown = widgets.Dropdown(\n",
    "    options=['dev', 'uat', 'prod'],\n",
    "    value='dev',\n",
    "    description='Environment:',\n",
    ")\n",
    "\n",
    "outputdirectory = widgets.Text(\"testdata\")\n",
    "\n",
    "# Display the widget\n",
    "display(widgets.HBox([env_dropdown, outputdirectory]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_env = env_dropdown.value\n",
    "outputdir = outputdirectory.value\n",
    "healthData = HealthData(selected_env,outputdir) # type: ignore\n",
    "healthData.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "healthdatadf = pd.read_parquet(f\"./{outputdir}/health_data_{selected_env}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####!pyspark --packages io.delta:delta-core_2.11:0.4.0\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"DeltaTableCreation\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_healthdatadf = healthdatadf[healthdatadf['BloodPressure'] > 120]\n",
    "print(filtered_healthdatadf)\n",
    "\n",
    "# Use Spark SQL to filter the data and convert to Pandas DataFrame\n",
    "spark.createDataFrame(healthdatadf).createOrReplaceTempView(\"sparkhealthdatadf\")\n",
    "filtered_healthdatadf_sql = spark.sql(\"SELECT * FROM sparkhealthdatadf WHERE BloodPressure > 120\").toPandas()\n",
    "print(filtered_healthdatadf_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame from the healthdatadf DataFrame\n",
    "sparkdf = spark.createDataFrame(healthdatadf)\n",
    "\n",
    "# Write the DataFrame as a Delta table\n",
    "sparkdf.write.format(\"delta\").mode(\"overwrite\").save(f\"./{outputdir}/health_data_{selected_env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Delta table\n",
    "delta_table = spark.read.format(\"delta\").load(f\"./{outputdir}/health_data_{selected_env}\")\n",
    "\n",
    "filtered_data = delta_table.filter(delta_table.BloodPressure > 120)\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "delta_table.createOrReplaceTempView(f\"health_data\")\n",
    "\n",
    "# Use Spark SQL to filter the data\n",
    "filtered_data_sql = spark.sql(\"SELECT * FROM health_data WHERE BloodPressure > 120\")\n",
    "filtered_data_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the SQL query and display the results\n",
    "result = spark.sql(\"SELECT * FROM `parquet`.`./testdata/health_data_dev` WHERE BloodPressure > 120\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List all files in the subfolders of the testdata directory\n",
    "file_list = [os.path.join(root, file) for root, dirs, files in os.walk(outputdir) for file in files if file.endswith('.csv') or file.endswith('.parquet')]\n",
    "\n",
    "# Create a dropdown widget for file selection\n",
    "file_dropdown = widgets.Dropdown(\n",
    "    options=[file.replace('testdata/', '') for file in file_list],\n",
    "    description='Files:',\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "display(file_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension= file_dropdown.value.split('.')[-1]\n",
    "\n",
    "spark.sql(f\"SELECT * FROM `{file_extension}`.`./{outputdir}/{file_dropdown.value}`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List columns and data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "display(df.describe().show())\n",
    "# Find the highest salary\n",
    "highest_salary = df.agg({\"Salary\": \"max\"}).collect()[0][0]\n",
    "print(f\"Highest Salary: {highest_salary}: with Education {df.filter(df.Salary == highest_salary).select('Education').collect()[0][0]} Position: {df.filter(df.Salary == highest_salary).select('Position').collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "database_name = file_dropdown.value.split('/')[0].replace('-', '')\n",
    "table_name = file_dropdown.value.split('/')[1].split('.')[0].replace('_', '')\n",
    "\n",
    "print(database_name)\n",
    "print(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "sparkdf = spark.read.format(f\"{file_extension}\").option(\"header\", \"true\").load(f\"./{outputdir}/{file_dropdown.value}\")\n",
    "\n",
    "# Rename columns to remove invalid characters\n",
    "for col in sparkdf.columns:\n",
    "\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('\\n', '').replace('\\t', '').replace('=', '')\n",
    "\tsparkdf = sparkdf.withColumnRenamed(col, new_col)\n",
    "\n",
    "# Create a database\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "# Use the created database\n",
    "spark.sql(f\"USE {database_name}\")\n",
    "\n",
    "# Write the DataFrame as a Delta table in the created database\n",
    "sparkdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{table_name}\")\n",
    "\n",
    "# Verify that the table has been created and data has been loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()\n",
    "#spark.sql(f\"SELECT * FROM {table_name}\").show()\n",
    "# Group by Location and show the count of each location\n",
    "spark.sql(f\"SELECT Location, COUNT(*) as count FROM {table_name} GROUP BY Location ORDER BY count DESC\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of people working from home\n",
    "work_from_home_count = spark.sql(f\"SELECT COUNT(*) as work_from_home FROM {table_name} WHERE Location like '%work from home%'\")\n",
    "work_from_home_count.show()\n",
    "# Count unique locations\n",
    "unique_locations_count = spark.sql(f\"SELECT COUNT(DISTINCT Location) as unique_locations FROM {table_name}\")\n",
    "unique_locations_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "locations_df = sparkdf.select(\"Location\").groupBy(\"Location\").count().orderBy(\"count\", ascending=False).toPandas()\n",
    "\n",
    "# Fill None values in the 'Location' column with an empty string\n",
    "locations_df['Location'] = locations_df['Location'].fillna('')\n",
    "\n",
    "# Plot the locations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(locations_df['Location'], locations_df['count'])\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Locations')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the DESCRIBE EXTENDED query\n",
    "describe_df = spark.sql(f\"DESCRIBE EXTENDED {table_name}\")\n",
    "\n",
    "# Convert to Pandas DataFrame for better display\n",
    "describe_pd_df = describe_df.toPandas()\n",
    "\n",
    "# Display the DataFrame\n",
    "display(describe_pd_df)\n",
    "\n",
    "location_data_type = describe_pd_df[describe_pd_df['col_name'] == 'Location']['data_type'].values[1]\n",
    "print(location_data_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
