{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "To run this Jupyter notebook, use Python 3.13 or later. The recommended environment is a virtual environment or a conda environment with the necessary packages installed, as specified int he requirements.txt file. Ensure that you have Jupyter Notebook installed to execute the cells interactively.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run installrequirements.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Reusable Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run reuse.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a dropdown widget for environment selection\n",
    "env_dropdown = widgets.Dropdown(\n",
    "    options=['dev', 'uat', 'prod'],\n",
    "    value='dev',\n",
    "    description='Environment:',\n",
    ")\n",
    "\n",
    "outputdirectory = widgets.Text(\"testdata\")\n",
    "\n",
    "# Display the widget\n",
    "display(widgets.HBox([env_dropdown, outputdirectory]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_env = env_dropdown.value\n",
    "outputdir = outputdirectory.value\n",
    "healthData = HealthData(selected_env,outputdir) # type: ignore\n",
    "healthData.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "healthdatadf = pd.read_parquet(f\"./{outputdir}/health_data_{selected_env}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####!pyspark --packages io.delta:delta-core_2.11:0.4.0\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"DeltaTableCreation_new\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_healthdatadf = healthdatadf[healthdatadf['BloodPressure'] > 120]\n",
    "print(filtered_healthdatadf)\n",
    "\n",
    "# Use Spark SQL to filter the data and convert to Pandas DataFrame\n",
    "spark.createDataFrame(healthdatadf).createOrReplaceTempView(\"sparkhealthdatadf\")\n",
    "filtered_healthdatadf_sql = spark.sql(\"SELECT * FROM sparkhealthdatadf WHERE BloodPressure > 120\").toPandas()\n",
    "print(filtered_healthdatadf_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame from the healthdatadf DataFrame\n",
    "sparkdf = spark.createDataFrame(healthdatadf)\n",
    "\n",
    "# Write the DataFrame as a Delta table\n",
    "sparkdf.write.format(\"delta\").mode(\"overwrite\").save(f\"./{outputdir}/health_data_{selected_env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Delta table\n",
    "delta_table = spark.read.format(\"delta\").load(f\"./{outputdir}/health_data_{selected_env}\")\n",
    "\n",
    "filtered_data = delta_table.filter(delta_table.BloodPressure > 120)\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "delta_table.createOrReplaceTempView(f\"health_data\")\n",
    "\n",
    "# Use Spark SQL to filter the data\n",
    "filtered_data_sql = spark.sql(\"SELECT * FROM health_data WHERE BloodPressure > 120\")\n",
    "filtered_data_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the SQL query and display the results\n",
    "result = spark.sql(\"SELECT * FROM `parquet`.`./testdata/health_data_dev` WHERE BloodPressure > 120\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List all files in the subfolders of the testdata directory\n",
    "file_list = [os.path.join(root, file) for root, dirs, files in os.walk(outputdir) for file in files if file.endswith('.csv') or file.endswith('.parquet')]\n",
    "\n",
    "# Create a dropdown widget for file selection\n",
    "file_dropdown = widgets.Dropdown(\n",
    "    options=[file.replace('testdata/', '') for file in file_list],\n",
    "    description='Files:',\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "display(file_dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension= file_dropdown.value.split('.')[-1]\n",
    "\n",
    "spark.sql(f\"SELECT * FROM `{file_extension}`.`./{outputdir}/{file_dropdown.value}`\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List columns and data types\n",
    "sparkdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "display(sparkdf.describe().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if '/' in file_dropdown.value:\n",
    "\tdatabase_name = file_dropdown.value.split('/')[0].replace('-', '') if file_dropdown.value.split('/')[0] else 'TestDB'\n",
    "\ttable_name = file_dropdown.value.split('/')[1].split('.')[0].replace('_', '')\n",
    "else:\n",
    "\tdatabase_name = 'TestDB'\n",
    "\ttable_name = file_dropdown.value.split('.')[0].replace('_', '')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "sparkdf = spark.read.format(f\"{file_extension}\").option(\"header\", \"true\").load(f\"./{outputdir}/{file_dropdown.value}\")\n",
    "\n",
    "# Rename columns to remove invalid characters\n",
    "for col in sparkdf.columns:\n",
    "\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('\\n', '').replace('\\t', '').replace('=', '')\n",
    "\tsparkdf = sparkdf.withColumnRenamed(col, new_col)\n",
    "\n",
    "# Create a database\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "# Use the created database\n",
    "spark.sql(f\"USE {database_name}\")\n",
    "\n",
    "table_name = table_name.replace(' ', '_').replace('(', '').replace(')', '').replace('\\n', '').replace('\\t', '').replace('=', '').replace('-', '')\n",
    "\n",
    "# Write the DataFrame as a Delta table in the created database\n",
    "sparkdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{table_name}\")\n",
    "\n",
    "# Verify that the table has been created and data has been loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT COUNT(*) FROM {table_name}\").show()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the count of rows in the table\n",
    "count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0]['count']\n",
    "\n",
    "# Plot the count\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([table_name], [count], color='blue')\n",
    "plt.xlabel('Table Name')\n",
    "plt.ylabel('Row Count')\n",
    "plt.title('Row Count in Table')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame for plotting\n",
    "pandas_df = sparkdf.toPandas()\n",
    "\n",
    "# Plot the distribution of BloodPressure\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(pandas_df['BloodPressure'], bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of Blood Pressure')\n",
    "plt.xlabel('Blood Pressure')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Age vs BloodPressure\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pandas_df['Age'], pandas_df['BloodPressure'], alpha=0.7)\n",
    "plt.title('Age vs Blood Pressure')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Blood Pressure')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
