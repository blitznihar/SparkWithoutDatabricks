{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "To run this Jupyter notebook, use Python 3.13 or later. The recommended environment is a virtual environment or a conda environment with the necessary packages installed, as specified int he requirements.txt file. Ensure that you have Jupyter Notebook installed to execute the cells interactively.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run installrequirements.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Reusable Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run reuseGenerateData.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a dropdown widget for environment selection\n",
    "env_dropdown = widgets.Dropdown(\n",
    "    options=[\"dev\", \"uat\", \"prod\"],\n",
    "    value=\"dev\",\n",
    "    description=\"Environment:\",\n",
    ")\n",
    "\n",
    "outputdirectory = widgets.Text(\"testdata\")\n",
    "\n",
    "# Display the widget\n",
    "display(widgets.HBox([env_dropdown, outputdirectory]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_env = env_dropdown.value\n",
    "outputdir = outputdirectory.value\n",
    "healthData = HealthData(selected_env, outputdir)  # type: ignore\n",
    "healthData.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "healthdatadf = pd.read_parquet(f\"./{outputdir}/health_data_{selected_env}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####!pyspark --packages io.delta:delta-core_2.11:0.4.0\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"DeltaTableCreation_new\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_healthdatadf = healthdatadf[healthdatadf[\"BloodPressure\"] > 120]\n",
    "print(filtered_healthdatadf)\n",
    "\n",
    "# Use Spark SQL to filter the data and convert to Pandas DataFrame\n",
    "spark.createDataFrame(healthdatadf).createOrReplaceTempView(\"sparkhealthdatadf\")\n",
    "filtered_healthdatadf_sql = spark.sql(\n",
    "    \"SELECT * FROM sparkhealthdatadf WHERE BloodPressure > 120\"\n",
    ").toPandas()\n",
    "print(filtered_healthdatadf_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame from the healthdatadf DataFrame\n",
    "sparkdf = spark.createDataFrame(healthdatadf)\n",
    "\n",
    "# Write the DataFrame as a Delta table\n",
    "sparkdf.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    f\"./{outputdir}/health_data_{selected_env}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Delta table\n",
    "delta_table = spark.read.format(\"delta\").load(\n",
    "    f\"./{outputdir}/health_data_{selected_env}\"\n",
    ")\n",
    "\n",
    "filtered_data = delta_table.filter(delta_table.BloodPressure > 120)\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "delta_table.createOrReplaceTempView(f\"health_data\")\n",
    "\n",
    "# Use Spark SQL to filter the data\n",
    "filtered_data_sql = spark.sql(\"SELECT * FROM health_data WHERE BloodPressure > 120\")\n",
    "filtered_data_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the SQL query and display the results\n",
    "result = spark.sql(\n",
    "    \"SELECT * FROM `parquet`.`./testdata/health_data_dev` WHERE BloodPressure > 120\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List all files in the subfolders of the testdata directory\n",
    "file_list = [\n",
    "    os.path.join(root, file)\n",
    "    for root, dirs, files in os.walk(outputdir)\n",
    "    for file in files\n",
    "    if file.endswith(\".csv\") or file.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "# Create a dropdown widget for file selection\n",
    "file_dropdown = widgets.Dropdown(\n",
    "    options=[file.replace(\"testdata/\", \"\") for file in file_list],\n",
    "    description=\"Files:\",\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "display(file_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension = file_dropdown.value.split(\".\")[-1]\n",
    "\n",
    "spark.sql(\n",
    "    f\"SELECT * FROM `{file_extension}`.`./{outputdir}/{file_dropdown.value}`\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List columns and data types\n",
    "sparkdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "display(sparkdf.describe().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"/\" in file_dropdown.value:\n",
    "    database_name = (\n",
    "        file_dropdown.value.split(\"/\")[0].replace(\"-\", \"\")\n",
    "        if file_dropdown.value.split(\"/\")[0]\n",
    "        else \"TestDB\"\n",
    "    )\n",
    "    table_name = file_dropdown.value.split(\"/\")[1].split(\".\")[0].replace(\"_\", \"\")\n",
    "else:\n",
    "    database_name = \"TestDB\"\n",
    "    table_name = file_dropdown.value.split(\".\")[0].replace(\"_\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "sparkdf = (\n",
    "    spark.read.format(f\"{file_extension}\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(f\"./{outputdir}/{file_dropdown.value}\")\n",
    ")\n",
    "\n",
    "# Rename columns to remove invalid characters\n",
    "for col in sparkdf.columns:\n",
    "    new_col = (\n",
    "        col.replace(\" \", \"_\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "        .replace(\"\\n\", \"\")\n",
    "        .replace(\"\\t\", \"\")\n",
    "        .replace(\"=\", \"\")\n",
    "    )\n",
    "    sparkdf = sparkdf.withColumnRenamed(col, new_col)\n",
    "\n",
    "# Create a database\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "# Use the created database\n",
    "spark.sql(f\"USE {database_name}\")\n",
    "\n",
    "table_name = (\n",
    "    table_name.replace(\" \", \"_\")\n",
    "    .replace(\"(\", \"\")\n",
    "    .replace(\")\", \"\")\n",
    "    .replace(\"\\n\", \"\")\n",
    "    .replace(\"\\t\", \"\")\n",
    "    .replace(\"=\", \"\")\n",
    "    .replace(\"-\", \"\")\n",
    ")\n",
    "\n",
    "# Write the DataFrame as a Delta table in the created database\n",
    "sparkdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{table_name}\")\n",
    "\n",
    "# Verify that the table has been created and data has been loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT COUNT(*) FROM {table_name}\").show()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the count of rows in the table\n",
    "count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0][\"count\"]\n",
    "\n",
    "# Plot the count\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([table_name], [count], color=\"blue\")\n",
    "plt.xlabel(\"Table Name\")\n",
    "plt.ylabel(\"Row Count\")\n",
    "plt.title(\"Row Count in Table\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
